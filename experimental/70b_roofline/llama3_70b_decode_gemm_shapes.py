#!/usr/bin/env python3
# EXPERIMENTAL CODE: NOT FOR OFFICIAL INFERENCEMAX
# GENERATED BY CLAUDE CODE
# VERY LIKELY THAT ANYTHING IN EXPERIMENTAL FOLDER IS WRONG

"""Print GEMM shapes for LLaMA 3 70B decode (single token generation)."""

import argparse

import torch

from nvMatmulHeuristics import (
    NvMatmulHeuristicsFlags,
    NvMatmulHeuristicsInterface,
    NvMatmulHeuristicsMatmulLayout,
    NvMatmulHeuristicsNvidiaGpu,
    NvMatmulHeuristicsTarget,
)

# LLaMA 3 70B architecture parameters
HIDDEN_SIZE = 8192
INTERMEDIATE_SIZE = 28672  # FFN intermediate size (SwiGLU)
NUM_LAYERS = 80

# GPU specs for roofline model
GPU_SPECS = {
    "H100_SXM": {
        "bw_gbps": 3350,  # GB/s HBM3 bandwidth
        "flops": 989e12,  # BF16 Tensor Core FLOPS (989 TFLOPS)
    },
    "H200_SXM": {
        "bw_gbps": 4800,  # GB/s HBM3e bandwidth
        "flops": 989e12,  # BF16 Tensor Core FLOPS (same as H100 SXM)
    },
    "B200": {
        "bw_gbps": 8000,  # GB/s HBM3e bandwidth
        "flops": 2250e12,  # BF16 Tensor Core FLOPS (2.25 PFLOPS)
    },
    "GB200_NVL": {
        "bw_gbps": 8000,  # GB/s HBM3e bandwidth (per GPU)
        "flops": 2500e12,  # BF16 Tensor Core FLOPS (2.5 PFLOPS per GPU)
    },
    "GB300_NVL": {
        "bw_gbps": 8000,  # GB/s HBM3e bandwidth (per GPU)
        "flops": 2500e12,  # BF16 Tensor Core FLOPS (2.5 PFLOPS per GPU)
    },
}

# Batch sizes to sweep
BATCH_SIZES = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048]


def setup_heuristics(gpu: NvMatmulHeuristicsNvidiaGpu):
    """Initialize nvMatmulHeuristics interface."""
    nvmmh = NvMatmulHeuristicsInterface(
        NvMatmulHeuristicsTarget.CUTLASS3,
        precision="HSH",
        flags=NvMatmulHeuristicsFlags.PERF_MODEL_BASED_AUTO_TUNING,
    )
    hw = nvmmh.createHardwareDescriptor()
    nvmmh.setHardwarePredefinedGpu(hw, gpu)
    layout = NvMatmulHeuristicsMatmulLayout.NN_ROW_MAJOR
    nvmmh.loadInternalDiscoverySet(layout, hw)
    return nvmmh, hw, layout


def estimate_gemm_runtime(nvmmh, hw, layout, m: int, n: int, k: int) -> float:
    """Estimate GEMM runtime in milliseconds using nvMatmulHeuristics."""
    configs = nvmmh.get_with_mnk(m, n, k, layout, 1, hw)
    if configs:
        return configs[0]["runtime"] * 1000  # Convert to ms
    return 0.0


def benchmark_torch_matmul(m: int, k: int, n: int, warmup: int = 50, iters: int = 200) -> float:
    """Benchmark torch.matmul runtime in milliseconds."""
    a = torch.randn(m, k, dtype=torch.bfloat16, device="cuda")
    b = torch.randn(k, n, dtype=torch.bfloat16, device="cuda")

    # L2 cache flush buffer (H100 has 50MB L2, use larger buffer to ensure flush)
    l2_flush = torch.empty(64 * 1024 * 1024, dtype=torch.int8, device="cuda")

    # Warmup
    for _ in range(warmup):
        torch.matmul(a, b)
    torch.cuda.synchronize()

    # Benchmark with L2 cache flush between iterations
    total_time = 0.0
    start = torch.cuda.Event(enable_timing=True)
    end = torch.cuda.Event(enable_timing=True)

    for _ in range(iters):
        l2_flush.zero_()  # Flush L2 cache
        start.record()
        torch.matmul(a, b)
        end.record()
        torch.cuda.synchronize()
        total_time += start.elapsed_time(end)

    return total_time / iters


def compute_roofline_sol(m: int, k: int, n: int, gpu_name: str):
    """Compute roofline Speed-of-Light (SOL) runtime for a GEMM operation.

    The roofline model gives the theoretical minimum runtime assuming either:
    - Memory bandwidth is the bottleneck (memory-bound)
    - Compute throughput is the bottleneck (compute-bound)

    Args:
        m, k, n: GEMM dimensions (M x K) @ (K x N)
        gpu_name: GPU name for specs lookup

    Returns:
        Tuple of (runtime_ms, bytes_transferred, flops, is_memory_bound)
    """
    specs = GPU_SPECS[gpu_name]
    bw_bytes_per_s = specs["bw_gbps"] * 1e9  # Convert GB/s to B/s
    peak_flops = specs["flops"]

    # Bytes: read A (M*K) + read B (K*N) + write C (M*N), all in BF16 (2 bytes)
    bytes_transferred = 2 * (m * k + k * n + m * n)

    # FLOPs: 2*M*N*K for matmul
    flops = 2 * m * n * k

    # Time to transfer all data at peak bandwidth
    time_memory_s = bytes_transferred / bw_bytes_per_s

    # Time to compute all FLOPs at peak throughput
    time_compute_s = flops / peak_flops

    # Roofline: runtime is max of memory time and compute time
    is_memory_bound = time_memory_s >= time_compute_s
    runtime_s = max(time_memory_s, time_compute_s)
    runtime_ms = runtime_s * 1000.0

    return runtime_ms, bytes_transferred, flops, is_memory_bound


def compute_gemm_metrics(m: int, k: int, n: int, runtime_ms: float, gpu_name: str):
    """Compute MBU and MFU for a GEMM operation.

    Args:
        m, k, n: GEMM dimensions (M x K) @ (K x N)
        runtime_ms: Runtime in milliseconds
        gpu_name: GPU name for specs lookup

    Returns:
        Tuple of (bytes_transferred, flops, mbu_percent, mfu_percent)
    """
    specs = GPU_SPECS[gpu_name]
    bw_gbps = specs["bw_gbps"]
    peak_flops = specs["flops"]

    # Bytes: read A (M*K) + read B (K*N) + write C (M*N), all in BF16 (2 bytes)
    bytes_transferred = 2 * (m * k + k * n + m * n)

    # FLOPs: 2*M*N*K for matmul
    flops = 2 * m * n * k

    runtime_s = runtime_ms / 1000.0

    # Achieved bandwidth and compute
    achieved_bw_gbps = (bytes_transferred / 1e9) / runtime_s if runtime_s > 0 else 0
    achieved_flops = flops / runtime_s if runtime_s > 0 else 0

    # MBU and MFU as percentages
    mbu = (achieved_bw_gbps / bw_gbps) * 100
    mfu = (achieved_flops / peak_flops) * 100

    return bytes_transferred, flops, mbu, mfu


def print_gemm_shapes(gpu: NvMatmulHeuristicsNvidiaGpu, gpu_name: str) -> None:
    """Print all GEMM shapes for LLaMA 3 70B decode step across batch sizes."""
    seq_len = 1  # Decode generates one token at a time
    specs = GPU_SPECS[gpu_name]

    print("LLaMA 3 70B Decode FFN GEMM Shapes")
    print("=" * 180)
    print(f"Hidden size: {HIDDEN_SIZE}")
    print(f"Intermediate size: {INTERMEDIATE_SIZE}")
    print(f"GPU: {gpu_name} (BW: {specs['bw_gbps']} GB/s, FLOPS: {specs['flops']/1e12:.0f} TFLOPS)")
    print(f"Batch sizes: {BATCH_SIZES}")
    print("=" * 180)
    print()

    # Initialize heuristics
    nvmmh, hw, layout = setup_heuristics(gpu)

    for batch_size in BATCH_SIZES:
        m = batch_size * seq_len

        print(f"Batch size: {batch_size} (M={m})")
        print("-" * 180)
        print(
            f"  {'Name':20} {'Shape':>21}  {'Roofline':>10}  {'Heuristic':>10}  {'Torch':>10}  "
            f"{'MBU(R)':>7}  {'MFU(R)':>7}  {'MBU(H)':>7}  {'MFU(H)':>7}  {'MBU(T)':>7}  {'MFU(T)':>7}"
        )
        print("-" * 180)

        gemms = [
            ("Gate Projection", (m, HIDDEN_SIZE, INTERMEDIATE_SIZE)),
            ("Up Projection", (m, HIDDEN_SIZE, INTERMEDIATE_SIZE)),
            ("Down Projection", (m, INTERMEDIATE_SIZE, HIDDEN_SIZE)),
        ]

        total_roof_runtime = 0.0
        total_heur_runtime = 0.0
        total_torch_runtime = 0.0
        total_bytes = 0
        total_flops = 0

        for name, (m_dim, k, n) in gemms:
            # Roofline SOL
            roof_runtime, bytes_xfer, flops, is_mem_bound = compute_roofline_sol(m_dim, k, n, gpu_name)
            # Heuristic
            heur_runtime = estimate_gemm_runtime(nvmmh, hw, layout, m_dim, n, k)
            # Torch benchmark
            torch_runtime = benchmark_torch_matmul(m_dim, k, n)

            # Compute metrics for each method
            _, _, mbu_r, mfu_r = compute_gemm_metrics(m_dim, k, n, roof_runtime, gpu_name)
            _, _, mbu_h, mfu_h = compute_gemm_metrics(m_dim, k, n, heur_runtime, gpu_name)
            _, _, mbu_t, mfu_t = compute_gemm_metrics(m_dim, k, n, torch_runtime, gpu_name)

            total_roof_runtime += roof_runtime
            total_heur_runtime += heur_runtime
            total_torch_runtime += torch_runtime
            total_bytes += bytes_xfer
            total_flops += flops

            print(
                f"  {name:20} {m_dim:5} x {k:5} x {n:5}  {roof_runtime:8.4f} ms  {heur_runtime:8.4f} ms  {torch_runtime:8.4f} ms  "
                f"{mbu_r:6.2f}%  {mfu_r:6.2f}%  {mbu_h:6.2f}%  {mfu_h:6.2f}%  {mbu_t:6.2f}%  {mfu_t:6.2f}%"
            )

        # Compute total MBU/MFU for roofline
        total_roof_s = total_roof_runtime / 1000.0
        total_roof_bw = (total_bytes / 1e9) / total_roof_s if total_roof_s > 0 else 0
        total_roof_flops = total_flops / total_roof_s if total_roof_s > 0 else 0
        total_mbu_r = (total_roof_bw / specs["bw_gbps"]) * 100
        total_mfu_r = (total_roof_flops / specs["flops"]) * 100

        # Compute total MBU/MFU for heuristic
        total_heur_s = total_heur_runtime / 1000.0
        total_heur_bw = (total_bytes / 1e9) / total_heur_s if total_heur_s > 0 else 0
        total_heur_flops = total_flops / total_heur_s if total_heur_s > 0 else 0
        total_mbu_h = (total_heur_bw / specs["bw_gbps"]) * 100
        total_mfu_h = (total_heur_flops / specs["flops"]) * 100

        # Compute total MBU/MFU for torch
        total_torch_s = total_torch_runtime / 1000.0
        total_torch_bw = (total_bytes / 1e9) / total_torch_s if total_torch_s > 0 else 0
        total_torch_flops_achieved = total_flops / total_torch_s if total_torch_s > 0 else 0
        total_mbu_t = (total_torch_bw / specs["bw_gbps"]) * 100
        total_mfu_t = (total_torch_flops_achieved / specs["flops"]) * 100

        print("-" * 180)
        print(
            f"  {'Total per layer':20} {' ':>21}  {total_roof_runtime:8.4f} ms  {total_heur_runtime:8.4f} ms  {total_torch_runtime:8.4f} ms  "
            f"{total_mbu_r:6.2f}%  {total_mfu_r:6.2f}%  {total_mbu_h:6.2f}%  {total_mfu_h:6.2f}%  {total_mbu_t:6.2f}%  {total_mfu_t:6.2f}%"
        )
        print()


GPU_CHOICES = {
    "H100_SXM": NvMatmulHeuristicsNvidiaGpu.H100_SXM,
    "H200_SXM": NvMatmulHeuristicsNvidiaGpu.H200_SXM,
    "B200": NvMatmulHeuristicsNvidiaGpu.B200,
    "GB200_NVL": NvMatmulHeuristicsNvidiaGpu.GB200_NVL,
    "GB300_NVL": NvMatmulHeuristicsNvidiaGpu.GB300_NVL,
}


def print_gpu_comparison() -> None:
    """Print comparison across all GPUs with Roofline SOL and Heuristics."""
    seq_len = 1

    print("LLaMA 3 70B Decode FFN GEMM - GPU Comparison (Roofline SOL & Heuristics)")
    print("=" * 120)
    print(f"Hidden size: {HIDDEN_SIZE}")
    print(f"Intermediate size: {INTERMEDIATE_SIZE}")
    print("=" * 120)

    gpu_names = list(GPU_CHOICES.keys())

    # Collect all data first
    # data[batch_size][gpu_name] = (roof_runtime, heur_runtime, roof_mbu, roof_mfu, heur_mbu, heur_mfu)
    data = {}
    for batch_size in BATCH_SIZES:
        m = batch_size * seq_len
        data[batch_size] = {}

        for gpu_name, gpu_enum in GPU_CHOICES.items():
            nvmmh, hw, layout = setup_heuristics(gpu_enum)
            specs = GPU_SPECS[gpu_name]

            gemms = [
                (m, HIDDEN_SIZE, INTERMEDIATE_SIZE),  # Gate
                (m, HIDDEN_SIZE, INTERMEDIATE_SIZE),  # Up
                (m, INTERMEDIATE_SIZE, HIDDEN_SIZE),  # Down
            ]

            total_roof_runtime = 0.0
            total_heur_runtime = 0.0
            total_bytes = 0
            total_flops = 0

            for m_dim, k, n in gemms:
                # Roofline SOL
                roof_runtime, bytes_xfer, flops, _ = compute_roofline_sol(m_dim, k, n, gpu_name)
                total_roof_runtime += roof_runtime
                # Heuristic
                heur_runtime = estimate_gemm_runtime(nvmmh, hw, layout, m_dim, n, k)
                total_heur_runtime += heur_runtime
                total_bytes += bytes_xfer
                total_flops += flops

            # Compute MBU/MFU for roofline
            roof_s = total_roof_runtime / 1000.0
            roof_bw = (total_bytes / 1e9) / roof_s if roof_s > 0 else 0
            roof_flops_achieved = total_flops / roof_s if roof_s > 0 else 0
            roof_mbu = (roof_bw / specs["bw_gbps"]) * 100
            roof_mfu = (roof_flops_achieved / specs["flops"]) * 100

            # Compute MBU/MFU for heuristic
            heur_s = total_heur_runtime / 1000.0
            heur_bw = (total_bytes / 1e9) / heur_s if heur_s > 0 else 0
            heur_flops_achieved = total_flops / heur_s if heur_s > 0 else 0
            heur_mbu = (heur_bw / specs["bw_gbps"]) * 100
            heur_mfu = (heur_flops_achieved / specs["flops"]) * 100

            data[batch_size][gpu_name] = (
                total_roof_runtime, total_heur_runtime,
                roof_mbu, roof_mfu, heur_mbu, heur_mfu
            )

    # Print Roofline Runtime table
    print()
    print("Roofline SOL Runtime (ms per FFN layer):")
    print("-" * 100)
    header = f"{'Batch':>6}"
    for gpu_name in gpu_names:
        header += f"  {gpu_name:>12}"
    print(header)
    print("-" * 100)
    for batch_size in BATCH_SIZES:
        row = f"{batch_size:>6}"
        for gpu_name in gpu_names:
            roof_runtime = data[batch_size][gpu_name][0]
            row += f"  {roof_runtime:>10.4f}ms"
        print(row)

    # Print Heuristic Runtime table
    print()
    print("Heuristic Runtime (ms per FFN layer):")
    print("-" * 100)
    print(header)
    print("-" * 100)
    for batch_size in BATCH_SIZES:
        row = f"{batch_size:>6}"
        for gpu_name in gpu_names:
            heur_runtime = data[batch_size][gpu_name][1]
            row += f"  {heur_runtime:>10.4f}ms"
        print(row)

    # Print Roofline MBU table
    print()
    print("Roofline SOL Memory Bandwidth Utilization (MBU %):")
    print("-" * 100)
    print(header)
    print("-" * 100)
    for batch_size in BATCH_SIZES:
        row = f"{batch_size:>6}"
        for gpu_name in gpu_names:
            roof_mbu = data[batch_size][gpu_name][2]
            row += f"  {roof_mbu:>11.2f}%"
        print(row)

    # Print Heuristic MBU table
    print()
    print("Heuristic Memory Bandwidth Utilization (MBU %):")
    print("-" * 100)
    print(header)
    print("-" * 100)
    for batch_size in BATCH_SIZES:
        row = f"{batch_size:>6}"
        for gpu_name in gpu_names:
            heur_mbu = data[batch_size][gpu_name][4]
            row += f"  {heur_mbu:>11.2f}%"
        print(row)

    # Print Roofline MFU table
    print()
    print("Roofline SOL Model FLOPS Utilization (MFU %):")
    print("-" * 100)
    print(header)
    print("-" * 100)
    for batch_size in BATCH_SIZES:
        row = f"{batch_size:>6}"
        for gpu_name in gpu_names:
            roof_mfu = data[batch_size][gpu_name][3]
            row += f"  {roof_mfu:>11.2f}%"
        print(row)

    # Print Heuristic MFU table
    print()
    print("Heuristic Model FLOPS Utilization (MFU %):")
    print("-" * 100)
    print(header)
    print("-" * 100)
    for batch_size in BATCH_SIZES:
        row = f"{batch_size:>6}"
        for gpu_name in gpu_names:
            heur_mfu = data[batch_size][gpu_name][5]
            row += f"  {heur_mfu:>11.2f}%"
        print(row)

    # Print GPU Specs
    print()
    print("GPU Specs:")
    print("-" * 80)
    for gpu_name in gpu_names:
        specs = GPU_SPECS[gpu_name]
        print(f"  {gpu_name:12}: BW={specs['bw_gbps']:>5} GB/s, FLOPS={specs['flops']/1e12:>5.0f} TFLOPS")


def main():
    parser = argparse.ArgumentParser(
        description="Print GEMM shapes for LLaMA 3 70B decode"
    )
    parser.add_argument(
        "--gpu",
        type=str,
        choices=list(GPU_CHOICES.keys()),
        default="H100_SXM",
        help="GPU type for runtime estimation (default: H100_SXM)",
    )
    parser.add_argument(
        "--compare",
        action="store_true",
        help="Compare heuristics across all GPUs (no torch benchmarks)",
    )
    args = parser.parse_args()

    if args.compare:
        print_gpu_comparison()
    else:
        print_gemm_shapes(GPU_CHOICES[args.gpu], args.gpu)


if __name__ == "__main__":
    main()

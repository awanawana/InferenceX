# Model-specific SGLang server configurations for disaggregated inference.
#
# Each top-level key is a MODEL_NAME value (must match the directory name under MODEL_DIR).
#
# To add a new model: add a new top-level entry following the same schema.
# No script changes are required.
#
# Schema:
#   <model-name>:
#     base_flags: str          # Common flags for both prefill and decode
#     mtp_flags: str           # Appended to decode when DECODE_MTP_SIZE > 0
#     dp_flags: str            # Appended when DP is enabled (prefill or decode)
#     prefill:
#       mem_fraction_static: float
#       disable_radix_cache: bool
#       dp:                              # Config when data-parallel attention is enabled
#         max_running_requests: int
#         chunked_prefill_size: str      # Can be integer or bash arithmetic expression
#         cuda_graph_bs: str             # Space-separated values
#       no_dp:                           # Config when data-parallel attention is disabled
#         max_running_requests: int
#         chunked_prefill_size: int
#         cuda_graph_bs_range: str       # "start-end" expanded via seq
#     decode:
#       mem_fraction_static: float
#       prefill_round_robin_balance: bool
#       dp:
#         max_running_requests: int
#         chunked_prefill_size: str
#         cuda_graph_bs_range: str
#       ep_only:                         # Config when EP is enabled but DP is disabled
#         max_running_requests: int
#         chunked_prefill_size: int
#         cuda_graph_bs_range: str
#       no_dp:
#         max_running_requests: int
#         chunked_prefill_size: int
#         cuda_graph_bs_range: str

DeepSeek-V3:
  base_flags: "--decode-log-interval 1000 --log-level warning --watchdog-timeout 3600 --ep-dispatch-algorithm fake --load-balance-method round_robin --kv-cache-dtype fp8_e4m3 --attention-backend aiter --disaggregation-transfer-backend mori"
  mtp_flags: "--speculative-algorithm NEXTN --speculative-eagle-topk 1"
  dp_flags: "--moe-a2a-backend mori --enable-dp-attention --moe-dense-tp-size 1 --enable-dp-lm-head"
  prefill:
    mem_fraction_static: 0.8
    disable_radix_cache: true
    dp:
      max_running_requests: 24
      chunked_prefill_size: "MORI_MAX_DISPATCH_TOKENS_PREFILL * PREFILL_TP_SIZE"
      cuda_graph_bs: "1 2 3"
    no_dp:
      max_running_requests: 128
      chunked_prefill_size: 262144
      cuda_graph_bs_range: "1-128"
  decode:
    mem_fraction_static: 0.85
    prefill_round_robin_balance: true
    dp:
      max_running_requests: 4096
      chunked_prefill_size: "MORI_MAX_DISPATCH_TOKENS_DECODE * DECODE_TP_SIZE"
      cuda_graph_bs_range: "1-160"
    ep_only:
      max_running_requests: 256
      chunked_prefill_size: 262144
      cuda_graph_bs_range: "1-256"
    no_dp:
      max_running_requests: 128
      chunked_prefill_size: 262144
      cuda_graph_bs_range: "1-128"

DeepSeek-V3-0324:
  base_flags: "--decode-log-interval 1000 --log-level warning --watchdog-timeout 3600 --ep-dispatch-algorithm fake --load-balance-method round_robin --kv-cache-dtype fp8_e4m3 --attention-backend aiter --disaggregation-transfer-backend mori"
  mtp_flags: "--speculative-algorithm NEXTN --speculative-eagle-topk 1"
  dp_flags: "--moe-a2a-backend mori --enable-dp-attention --moe-dense-tp-size 1 --enable-dp-lm-head"
  prefill:
    mem_fraction_static: 0.8
    disable_radix_cache: true
    dp:
      max_running_requests: 24
      chunked_prefill_size: "MORI_MAX_DISPATCH_TOKENS_PREFILL * PREFILL_TP_SIZE"
      cuda_graph_bs: "1 2 3"
    no_dp:
      max_running_requests: 128
      chunked_prefill_size: 262144
      cuda_graph_bs_range: "1-128"
  decode:
    mem_fraction_static: 0.85
    prefill_round_robin_balance: true
    dp:
      max_running_requests: 4096
      chunked_prefill_size: "MORI_MAX_DISPATCH_TOKENS_DECODE * DECODE_TP_SIZE"
      cuda_graph_bs_range: "1-160"
    ep_only:
      max_running_requests: 256
      chunked_prefill_size: 262144
      cuda_graph_bs_range: "1-256"
    no_dp:
      max_running_requests: 128
      chunked_prefill_size: 262144
      cuda_graph_bs_range: "1-128"

DeepSeek-R1:
  base_flags: "--decode-log-interval 1000 --log-level warning --watchdog-timeout 3600 --ep-dispatch-algorithm fake --load-balance-method round_robin --kv-cache-dtype fp8_e4m3 --attention-backend aiter --disaggregation-transfer-backend mori"
  mtp_flags: "--speculative-algorithm NEXTN --speculative-eagle-topk 1"
  dp_flags: "--moe-a2a-backend mori --enable-dp-attention --moe-dense-tp-size 1 --enable-dp-lm-head"
  prefill:
    mem_fraction_static: 0.8
    disable_radix_cache: true
    dp:
      max_running_requests: 24
      chunked_prefill_size: "MORI_MAX_DISPATCH_TOKENS_PREFILL * PREFILL_TP_SIZE"
      cuda_graph_bs: "1 2 3"
    no_dp:
      max_running_requests: 128
      chunked_prefill_size: 262144
      cuda_graph_bs_range: "1-128"
  decode:
    mem_fraction_static: 0.85
    prefill_round_robin_balance: true
    dp:
      max_running_requests: 4096
      chunked_prefill_size: "MORI_MAX_DISPATCH_TOKENS_DECODE * DECODE_TP_SIZE"
      cuda_graph_bs_range: "1-160"
    ep_only:
      max_running_requests: 256
      chunked_prefill_size: 262144
      cuda_graph_bs_range: "1-256"
    no_dp:
      max_running_requests: 128
      chunked_prefill_size: 262144
      cuda_graph_bs_range: "1-128"

DeepSeek-R1-0528-MXFP4-Preview:
  base_flags: "--decode-log-interval 1000 --log-level warning --watchdog-timeout 3600 --ep-dispatch-algorithm fake --load-balance-method round_robin --kv-cache-dtype fp8_e4m3 --attention-backend aiter"
  mtp_flags: "--speculative-algorithm NEXTN --speculative-eagle-topk 1"
  dp_flags: "--moe-a2a-backend mori --enable-dp-attention --moe-dense-tp-size 1 --enable-dp-lm-head"
  prefill:
    mem_fraction_static: 0.8
    disable_radix_cache: true
    dp:
      max_running_requests: 24
      chunked_prefill_size: 16384
      cuda_graph_bs: "1 2 3"
    no_dp:
      max_running_requests: 128
      chunked_prefill_size: 16384
      cuda_graph_bs_range: "1-128"
  decode:
    mem_fraction_static: 0.85
    prefill_round_robin_balance: true
    dp:
      max_running_requests: 4096
      chunked_prefill_size: "MORI_MAX_DISPATCH_TOKENS_DECODE * DECODE_TP_SIZE"
      cuda_graph_bs_range: "1-160"
    ep_only:
      max_running_requests: 256
      chunked_prefill_size: 262144
      cuda_graph_bs_range: "1-256"
    no_dp:
      max_running_requests: 128
      chunked_prefill_size: 262144
      cuda_graph_bs_range: "1-128"
